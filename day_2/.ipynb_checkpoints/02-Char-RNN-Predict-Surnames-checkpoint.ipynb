{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from vocabulary import Vocabulary\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "START_TOKEN = \"^\"\n",
    "END_TOKEN = \"_\"\n",
    "\n",
    "IGNORE_INDEX_VALUE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions \n",
    "\n",
    "Data Model:\n",
    "- Raw data\n",
    "- Vectorizer\n",
    "- Vectorized Data\n",
    "- Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawSurnames(object):\n",
    "    def __init__(self, data_path, delimiter=\",\"):\n",
    "        self.data = pd.read_csv(data_path, delimiter=delimiter)\n",
    "\n",
    "    def get_data(self, filter_to_nationality=None):\n",
    "        if filter_to_nationality is not None:\n",
    "            return self.data[self.data.nationality.isin(filter_to_nationality)]\n",
    "        return self.data\n",
    "\n",
    "# vectorizer\n",
    "\n",
    "class SurnamesVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab, max_seq_length):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def save(self, filename):\n",
    "        vec_dict = {\"surname_vocab\": self.surname_vocab.get_serializable_contents(),\n",
    "                    \"nationality_vocab\": self.nationality_vocab.get_serializable_contents(),\n",
    "                    'max_seq_length': self.max_seq_length}\n",
    "\n",
    "        with open(filename, \"w\") as fp:\n",
    "            json.dump(vec_dict, fp)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, filename):\n",
    "        with open(filename, \"r\") as fp:\n",
    "            vec_dict = json.load(fp)\n",
    "\n",
    "        vec_dict[\"surname_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"surname_vocab\"])\n",
    "        vec_dict[\"nationality_vocab\"] = Vocabulary.deserialize_from_contents(vec_dict[\"nationality_vocab\"])\n",
    "        return cls(**vec_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def fit(cls, surname_df):\n",
    "        surname_vocab = Vocabulary(use_unks=False,\n",
    "                                   use_mask=True,\n",
    "                                   use_start_end=True,\n",
    "                                   start_token=START_TOKEN,\n",
    "                                   end_token=END_TOKEN)\n",
    "\n",
    "        nationality_vocab = Vocabulary(use_unks=False, use_start_end=False, use_mask=False)\n",
    "\n",
    "        max_seq_length = 0\n",
    "        for index, row in surname_df.iterrows():\n",
    "            surname_vocab.add_many(row.surname)\n",
    "            nationality_vocab.add(row.nationality)\n",
    "\n",
    "            if len(row.surname) > max_seq_length:\n",
    "                max_seq_length = len(row.surname)\n",
    "        max_seq_length = max_seq_length + 2\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab, max_seq_length)\n",
    "\n",
    "    @classmethod\n",
    "    def fit_transform(cls, surname_df, split='train'):\n",
    "        vectorizer = cls.fit(surname_df)\n",
    "        return vectorizer, vectorizer.transform(surname_df, split)\n",
    "\n",
    "    def transform(self, surname_df, split='train'):\n",
    "\n",
    "        df = surname_df[surname_df.split==split].reset_index()\n",
    "        n_data = len(df)\n",
    "        \n",
    "        x_surnames = np.zeros((n_data, self.max_seq_length), dtype=np.int64)\n",
    "        y_surnames = np.ones((n_data, self.max_seq_length), dtype=np.int64) * IGNORE_INDEX_VALUE\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            vectorized_surname = list(self.surname_vocab.map(row.surname, \n",
    "                                                             include_start_end=True))\n",
    "            x_part = vectorized_surname[:-1]\n",
    "            y_part = vectorized_surname[1:]\n",
    "            x_surnames[index, :len(x_part)] = x_part\n",
    "            y_surnames[index, :len(y_part)] = y_part\n",
    "\n",
    "        return VectorizedSurnames(x_surnames, y_surnames)\n",
    "\n",
    "# vec data\n",
    "\n",
    "class VectorizedSurnames(Dataset):\n",
    "    def __init__(self, x_surnames, y_surnames):\n",
    "        self.x_surnames = x_surnames\n",
    "        self.y_surnames = y_surnames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_surnames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x_surnames': self.x_surnames[index],\n",
    "                'y_surnames': self.y_surnames[index],\n",
    "                'x_lengths': len(self.x_surnames[index].nonzero()[0])}\n",
    "\n",
    "# data generator\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_parameter(*size):\n",
    "    out = torch.randn(*size, requires_grad=True, dtype=torch.float32)\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return nn.Parameter(out)\n",
    "\n",
    "def column_gather(y_out, x_lengths):\n",
    "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
    "\n",
    "    More precisely, iterate over batch row indices, get the vector that's at\n",
    "    the position indicated by the corresponding value in `x_lengths` at the row\n",
    "    index.\n",
    "\n",
    "    Args:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, sequence, feature)\n",
    "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
    "            shape: (batch,)\n",
    "\n",
    "    Returns:\n",
    "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
    "            shape: (batch, feature)\n",
    "    '''\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class ExplicitRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ExplicitRNN, self).__init__()\n",
    "        self.W_in2hid = new_parameter(input_size, hidden_size)\n",
    "        self.W_hid2hid = new_parameter(hidden_size, hidden_size)\n",
    "            \n",
    "        self.b_hid = new_parameter(1, hidden_size)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def _compute_next_hidden(self, x, h):\n",
    "        return F.tanh(x.matmul(self.W_in2hid) + \n",
    "                      h.matmul(self.W_hid2hid) + \n",
    "                      self.b_hid)\n",
    "\n",
    "    def forward(self, x_in, hid_t=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "\n",
    "        hiddens = []\n",
    "        if hid_t is None:\n",
    "            hid_t = torch.ones((batch_size, self.hidden_size))\n",
    "        \n",
    "        if x_in.is_cuda:\n",
    "            hid_t = hid_t.cuda()\n",
    "            \n",
    "        for t in range(seq_size):\n",
    "            x_t = x_in[t]\n",
    "            hid_t = self._compute_next_hidden(x_t, hid_t)\n",
    "            \n",
    "            hiddens.append(hid_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "\n",
    "        return hiddens\n",
    "    \n",
    "    \n",
    "    \n",
    "class CharNN(nn.Module):\n",
    "    def __init__(self, embedding_size, in_vocab_size, out_vocab_size, hidden_size, \n",
    "                 batch_first=False):\n",
    "        super(CharNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(embedding_dim=embedding_size, \n",
    "                                num_embeddings=in_vocab_size,\n",
    "                                padding_idx=0)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=out_vocab_size)\n",
    "        self.rnn = ExplicitRNN(input_size=embedding_size, \n",
    "                               hidden_size=hidden_size, \n",
    "                               batch_first=batch_first)\n",
    "    \n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        # x_in.shape == (batch_size, max_seq_length)\n",
    "        x_in = self.emb(x_in)\n",
    "        # x_in.shape == batch_size, max_seq_length, embedding_size\n",
    "        y_out = self.rnn(x_in)\n",
    "        # y_out.shape == batch_size, max_seq_lenth, hidden_size)\n",
    "\n",
    "        \n",
    "        #reshape into a mtrix so we can apply a linear layer.\n",
    "        dim0, dim1, dim2 = y_out.size()\n",
    "        y_out = y_out.contiguous().view(-1, dim2)\n",
    "\n",
    "        #Now that its a matrix, can apply liear layer\n",
    "        y_out = self.fc(y_out)\n",
    "\n",
    "        # optionally apply the softmax\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        y_out = y_out.view(dim0, dim1, -1)\n",
    "        #y_out.shape == (batch_size, max_seq_length, character_vocab_size)\n",
    "        return y_out\n",
    "    \n",
    "def normalize_sizes(net_output, y_true):\n",
    "    net_output = net_output.cpu()\n",
    "    y_true = y_true.cpu()\n",
    "    if len(net_output.size()) == 3:\n",
    "        net_output.contiguous()\n",
    "        net_output = net_output.view(-1, net_output.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true.contiguous()\n",
    "        y_true = y_true.view(-1)\n",
    "    return net_output, y_true\n",
    "\n",
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make, Train, and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    surname_csv=\"../data/surnames.csv\",\n",
    "    batch_size = 128,\n",
    "    cuda=True,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    load_zoo_model=True,\n",
    "    zoo={\n",
    "        'filename': '../modelzoo/charnn_emb16_hid64_surnames_predict.state',\n",
    "        'vocab': '../modelzoo/surnames_classify.vocab',\n",
    "        'comments': 'pre-trained surname sequence prediction (& generation model)',\n",
    "        'parameters': {\n",
    "            'embedding_size': 16,\n",
    "            'hidden_size': 64\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: set this to false to learn from scratch!\n",
    "# args.load_zoo_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectorizer!\n",
      "Loading state dict!\n"
     ]
    }
   ],
   "source": [
    "raw_data = RawSurnames(args.surname_csv).get_data()\n",
    "\n",
    "if os.path.exists(args.zoo['vocab']):\n",
    "    vectorizer = SurnamesVectorizer.load(args.zoo['vocab'])\n",
    "    print(\"Loading vectorizer!\")\n",
    "else:\n",
    "    vectorizer = SurnamesVectorizer.fit(raw_data)\n",
    "    print(\"Creating a new vectorizer.\")\n",
    "    \n",
    "train_dataset = vectorizer.transform(raw_data, split='train')\n",
    "test_dataset = vectorizer.transform(raw_data, split='test')\n",
    "\n",
    "zoo_params = args.zoo['parameters']\n",
    "\n",
    "net = CharNN(embedding_size=zoo_params['embedding_size'], \n",
    "             hidden_size=zoo_params['hidden_size'],\n",
    "             in_vocab_size=len(vectorizer.surname_vocab), \n",
    "             out_vocab_size=len(vectorizer.surname_vocab), \n",
    "             batch_first=True)\n",
    "\n",
    "if args.load_zoo_model and os.path.exists(args.zoo['filename']):\n",
    "    print(\"Loading state dict!\")\n",
    "    net.load_state_dict(torch.load(args.zoo['filename'], \n",
    "                                   map_location=lambda storage, loc: storage))\n",
    "else:\n",
    "    print(\"Using newly initiated network!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3fe06d550f4f8eb22e1ed150f88852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epochs'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0cf11a6ab64bf39397321d8fc20538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='training', max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b216bb01e6384af99f8c560ed07ef982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test', max=31), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "net = net.to(args.device)\n",
    "    \n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n",
    "\n",
    "# loss function\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)\n",
    "\n",
    "# progress bars\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='epochs', total=args.num_epochs, position=1)\n",
    "\n",
    "num_train_batches = len(train_dataset) // args.batch_size\n",
    "train_bar = tqdm_notebook(desc='training', total=num_train_batches, position=2)\n",
    "\n",
    "num_test_batches = len(test_dataset) // args.batch_size\n",
    "test_bar = tqdm_notebook(desc='test', total=num_test_batches, position=3)\n",
    "\n",
    "# history\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "\n",
    "\n",
    "try:\n",
    "    for _ in range(args.num_epochs):\n",
    "        batch_generator = generate_batches(train_dataset, batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "        \n",
    "        net.train()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            # step 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2\n",
    "            y_pred = net(batch_dict['x_surnames'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_surnames']\n",
    "            \n",
    "            # step 3\n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            \n",
    "            # step 4\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "          \n",
    "            # bonus steps: bookkeeping\n",
    "            \n",
    "            per_epoch_loss.append(loss.item())\n",
    "            \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_surnames'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            train_bar.update()\n",
    "            \n",
    "            train_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                  accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        train_loss_history.append(np.mean(per_epoch_loss))\n",
    "        train_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # loop over test dataset\n",
    "        \n",
    "        batch_generator = generate_batches(test_dataset, batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        \n",
    "        per_epoch_loss = []\n",
    "        per_epoch_accuracy = []\n",
    "            \n",
    "        # set it to eval mode; this turns stochastic functions off\n",
    "        net.eval()\n",
    "            \n",
    "        for batch_dict in batch_generator:\n",
    "            \n",
    "            # step 1: compute output\n",
    "            y_pred = net(batch_dict['x_surnames'], batch_dict['x_lengths'])\n",
    "            y_target = batch_dict['y_surnames']\n",
    "            \n",
    "            # step 2: compute metrics\n",
    "            \n",
    "            loss = sequence_loss(y_pred, y_target, IGNORE_INDEX_VALUE)\n",
    "            per_epoch_loss.append(loss.item())\n",
    "          \n",
    "            accuracy = compute_accuracy(y_pred, batch_dict['y_surnames'], IGNORE_INDEX_VALUE)\n",
    "            per_epoch_accuracy.append(accuracy)\n",
    "\n",
    "            test_bar.update()\n",
    "            \n",
    "            test_bar.set_postfix(loss=per_epoch_loss[-1], \n",
    "                                 accuracy=per_epoch_accuracy[-1])\n",
    "            \n",
    "        test_loss_history.append(np.mean(per_epoch_loss))\n",
    "        test_accuracy_history.append(np.mean(per_epoch_accuracy))\n",
    "        \n",
    "        # update bars\n",
    "        \n",
    "        epoch_bar.set_postfix(train_loss=train_loss_history[-1], \n",
    "                              train_accuracy=train_accuracy_history[-1],\n",
    "                              test_loss=test_loss_history[-1],\n",
    "                              test_accuracy=test_accuracy_history[-1])\n",
    "        epoch_bar.update()\n",
    "        test_bar.n = 0\n",
    "        train_bar.n = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(net.state_dict(), '02-Char-RNN-Predict-Surnames.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise!\n",
    "\n",
    "Now that we have a model which was trained to predict sequences, let's make our own sampler!\n",
    "\n",
    "The sampler will walk through the generation procedure, selecting one character a time.  The result is something like this: \n",
    "\n",
    "```\n",
    "['Poldtoff',\n",
    " 'Schestars',\n",
    " 'Gordoud',\n",
    " 'Kinsen',\n",
    " 'Venzey',\n",
    " 'Tumali',\n",
    " 'Pets',\n",
    " 'Aänchekin',\n",
    " 'GDigkov',\n",
    " 'Shadonov',\n",
    " 'Boulyanson',\n",
    " 'Gwae',\n",
    " 'Zgerege',\n",
    " 'Foxchevtsev',\n",
    " 'Progkin',\n",
    " 'Ussin']\n",
    "```\n",
    "\n",
    "\n",
    "see dl4nlp.info for more information on this exercise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
